import sys
import os
import json
import time

# Add the parent directory to sys.path to import the LLM module
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from LLM.openai_based_api import process_content

def load_agent_tools():
    """åŠ è½½Agentå·¥å…·å®šä¹‰"""
    try:
        tools_file = os.path.join(os.path.dirname(__file__), 'Agent_Tools.json')
        with open(tools_file, 'r', encoding='utf-8') as f:
            tools_data = json.load(f)
        return tools_data['tools']
    except Exception as e:
        print(f"Error loading agent tools: {e}")
        return None

def mock_tool_function(tool_name, arguments):
    """æ¨¡æ‹Ÿå·¥å…·å‡½æ•°çš„æ‰§è¡Œç»“æœ"""
    print(f"\nğŸ”§ æ¨¡æ‹Ÿæ‰§è¡Œå·¥å…·: {tool_name}")
    print(f"ğŸ“‹ å‚æ•°: {json.dumps(arguments, ensure_ascii=False, indent=2)}")
    
    # æ ¹æ®ä¸åŒçš„å·¥å…·è¿”å›ä¸åŒçš„æ¨¡æ‹Ÿç»“æœ
    if tool_name == "full_text_search":
        return {
            "results": [
                {"title": "æ–‡æ¡£1", "content": f"è¿™æ˜¯å…³äº'{arguments.get('query', '')}'çš„å…¨æ–‡æœç´¢ç»“æœç¤ºä¾‹", "score": 0.95},
                {"title": "æ–‡æ¡£2", "content": f"å¦ä¸€ä¸ªåŒ…å«'{arguments.get('query', '')}'çš„æ–‡æ¡£å†…å®¹", "score": 0.87}
            ],
            "total": 2
        }
    elif tool_name == "keyword_search":
        keywords = arguments.get('keywords', [])
        return {
            "results": [
                {"title": "å…³é”®è¯æ–‡æ¡£1", "content": f"è¿™æ˜¯åŒ…å«å…³é”®è¯{keywords}çš„ç²¾ç¡®åŒ¹é…ç»“æœ", "score": 1.0},
                {"title": "å…³é”®è¯æ–‡æ¡£2", "content": f"å¦ä¸€ä¸ªåŒ¹é…{keywords}çš„æ–‡æ¡£", "score": 0.92}
            ],
            "total": 2
        }
    elif tool_name == "vector_search":
        return {
            "results": [
                {"title": "è¯­ä¹‰æ–‡æ¡£1", "content": f"è¿™æ˜¯ä¸'{arguments.get('query', '')}'è¯­ä¹‰ç›¸å…³çš„å†…å®¹", "similarity": 0.89},
                {"title": "è¯­ä¹‰æ–‡æ¡£2", "content": f"å¦ä¸€ä¸ªè¯­ä¹‰ç›¸ä¼¼çš„æ–‡æ¡£å†…å®¹", "similarity": 0.82}
            ],
            "total": 2
        }
    else:
        return {"error": f"Unknown tool: {tool_name}"}

def simulate_conversation_with_tools(provider_name, model, system_prompt, user_query, tools, max_rounds=3):
    """æ¨¡æ‹Ÿå¸¦æœ‰å·¥å…·è°ƒç”¨çš„å¯¹è¯"""
    print(f"\n{'='*80}")
    print(f"ğŸ¤– å¼€å§‹Agentå¯¹è¯æ¨¡æ‹Ÿ")
    print(f"ğŸ“ ç”¨æˆ·æŸ¥è¯¢: {user_query}")
    print(f"{'='*80}")
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": user_query})
    
    for round_num in range(max_rounds):
        print(f"\n--- ç¬¬ {round_num + 1} è½®å¯¹è¯ ---")
        
        # è°ƒç”¨LLM
        response = process_content(
            provider_name=provider_name,
            user_prompt=user_query if round_num == 0 else "",
            model=model,
            system_prompt=system_prompt if round_num == 0 else None,
            tools=tools,
            tool_choice="auto"
        )
        
        print(f"ğŸ’¬ æ¨¡å‹å“åº”: {response['result']}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if 'tool_calls' in response and response['tool_calls']:
            print(f"\nğŸ› ï¸  æ£€æµ‹åˆ° {len(response['tool_calls'])} ä¸ªå·¥å…·è°ƒç”¨")
            
            # å¤„ç†æ¯ä¸ªå·¥å…·è°ƒç”¨
            tool_messages = []
            for tool_call in response['tool_calls']:
                tool_name = tool_call.function.name
                try:
                    arguments = json.loads(tool_call.function.arguments)
                except json.JSONDecodeError:
                    arguments = {}
                
                # æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œ
                tool_result = mock_tool_function(tool_name, arguments)
                print(f"âœ… å·¥å…·ç»“æœ: {json.dumps(tool_result, ensure_ascii=False, indent=2)}")
                
                # æ·»åŠ å·¥å…·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
                tool_messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": json.dumps(tool_result, ensure_ascii=False)
                })
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­å¯¹è¯è®©æ¨¡å‹å¤„ç†å·¥å…·ç»“æœ
            if tool_messages:
                # æ„å»ºåŒ…å«å·¥å…·è°ƒç”¨å†å²çš„æ¶ˆæ¯
                messages.append({
                    "role": "assistant", 
                    "content": response['result'],
                    "tool_calls": response['tool_calls']
                })
                messages.extend(tool_messages)
                
                # è®©æ¨¡å‹å¤„ç†å·¥å…·ç»“æœ
                follow_up_response = process_content(
                    provider_name=provider_name,
                    user_prompt="",  # ç©ºæŸ¥è¯¢ï¼Œè®©æ¨¡å‹åŸºäºå·¥å…·ç»“æœç»§ç»­
                    model=model,
                    tools=tools
                )
                
                print(f"\nğŸ¯ åŸºäºå·¥å…·ç»“æœçš„æœ€ç»ˆå›ç­”: {follow_up_response['result']}")
                break
        else:
            print("\nâœ¨ å¯¹è¯å®Œæˆï¼Œæ— éœ€å·¥å…·è°ƒç”¨")
            break
    
    print(f"\n{'='*80}")
    return response

def run_agent_tests():
    """è¿è¡ŒAgentæµ‹è¯•å¥—ä»¶"""
    # é…ç½®
    provider_name = "YUNWU-Dev"
    model = "gpt-4.1-2025-04-14"
    
    # åŠ è½½å·¥å…·å®šä¹‰
    tools = load_agent_tools()
    if not tools:
        print("âŒ æ— æ³•åŠ è½½å·¥å…·å®šä¹‰ï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(tools)} ä¸ªå·¥å…·:")
    for tool in tools:
        print(f"   - {tool['function']['name']}: {tool['function']['description']}")
    
    # ç³»ç»Ÿæç¤ºè¯
    system_prompt = """# Identity
You are an AI assistant, developed by AweMinds Technology Co., Ltd. 
Your primary role is to help customers by answering questions about content in the knowledge base which you can fully access by tool calls. 

# Instructions
## goals
Answer the user's request using the relevant tool(s), if they are available. Check that all the required parameters for each tool call are provided or can reasonably be inferred from context. IF there are no relevant tools or there are missing values for required parameters, ask the user to supply these values; otherwise proceed with the tool calls. If the user provides a specific value for a parameter (for example provided in quotes), make sure to use that value EXACTLY. DO NOT make up values for or ask about optional parameters. Carefully analyze descriptive terms in the request as they may indicate required parameter values that should be included even if not explicitly quoted.

## tool_calling
You have tools at your disposal to solve the user's query. Follow these rules regarding tool calls:
1. ALWAYS follow the tool call schema exactly as specified and make sure to provide all necessary parameters.
2. The conversation may reference tools that are no longer available. NEVER call tools that are not explicitly provided.
3. NEVER refer to tool names when speaking to the USER. Instead, just say what the tool is doing in natural language.
4. After receiving tool results, carefully reflect on their quality and determine optimal next steps before proceeding. Use your thinking to plan and iterate based on this new information, and then take the best next action. Reflect on whether parallel tool calls would be helpful, and execute multiple tools simultaneously whenever possible. Avoid slow sequential tool calls when not necessary.
5. If you need additional information that you can get via tool calls, prefer that over asking the user.
6. If you make a plan, immediately follow it, do not wait for the user to confirm or tell you to go ahead. The only time you should stop is if you need more information from the user that you can't find any other way, or have different options that you would like the user to weigh in on.
7. The vast majority of tools provided to you support multi-parameter combinations, such as vector searches using 5 or more keywords/phrases. If you wish to attempt multi-angle searches by providing multiple sets of parameters to obtain more comprehensive results, this is encouraged.

## maximize_parallel_tool_calls
CRITICAL: Execute multiple tool calls simultaneously whenever possible for maximum efficiency.
**When to use parallel calls:**
- Multiple searches with different keywords/methods
- Gathering information from various sources
- Any operations that don't depend on each other's output
**Process:**
1. Plan all needed information upfront
2. Execute all relevant tools together
3. Only use sequential calls when one tool's output is required for the next tool's input
**Default behavior:** Parallel execution unless operations are genuinely interdependent.

## search_and_reading
If you are unsure about the answer to the USER's request or how to satisfy their request, you should gather more information. This can be done with additional tool calls, asking clarifying questions, etc...
For example, if you've performed a semantic search, and the results may not fully answer the USER's request, or warrant additional research, feel free to call more tools.
If you've performed an answer that may partially satisfy the USER's query, but you're not confident, gather more information or use more tools before ending your turn.
Bias towards not asking the user for help if you can find the answer yourself."""

    # å¤æ‚æµ‹è¯•ç”¨ä¾‹ - è¯„ä¼°RAGç³»ç»Ÿåœ¨å¤æ‚åœºæ™¯ä¸‹çš„è¡¨ç°
    test_cases = [
        {
            "name": "èº«ä»½æµ‹è¯•",
            "query": "ä½ èƒ½å¹²ä»€ä¹ˆï¼Ÿ",
            "expected_tools": []
        },
        {
            "name": "å¤æ‚å› æœé“¾æ¨ç†",
            "query": "å¦‚æœå…¬å¸Aåœ¨2020å¹´æ”¶è´­äº†å…¬å¸Bï¼Œè€Œå…¬å¸Bä¹‹å‰åœ¨2018å¹´ä¸å…¬å¸Cç­¾ç½²äº†ç‹¬å®¶ä¾›åº”åè®®ï¼Œé‚£ä¹ˆåœ¨2021å¹´å…¬å¸Aæ¨å‡ºæ–°äº§å“æ—¶ï¼Œè¿™ä¸ªç‹¬å®¶ä¾›åº”åè®®å¯¹å…¶å¸‚åœºç­–ç•¥ä¼šäº§ç”Ÿä»€ä¹ˆå½±å“ï¼Ÿè¯·åˆ†æè‡³å°‘ä¸‰ä¸ªå±‚é¢çš„è¿é”ååº”ã€‚",
            "expected_tools": ["full_text_search", "keyword_search", "vector_search"]
        },
        {
            "name": "å¤šæºæ•°æ®çŸ›ç›¾å¤„ç†",
            "query": "æ–‡æ¡£Aæ˜¾ç¤ºæŸè¯ç‰©çš„ä¸´åºŠè¯•éªŒæˆåŠŸç‡ä¸º78%ï¼Œæ–‡æ¡£Bæ˜¾ç¤ºä¸º65%ï¼Œæ–‡æ¡£Cæ˜¾ç¤ºä¸º82%ã€‚è¯·åˆ†æè¿™äº›æ•°æ®å·®å¼‚çš„å¯èƒ½åŸå› ï¼Œå¹¶åˆ¤æ–­å“ªä¸ªæ•°æ®æ›´å¯ä¿¡ï¼ŒåŒæ—¶è§£é‡Šä½ çš„åˆ¤æ–­ä¾æ®ã€‚",
            "expected_tools": ["full_text_search", "keyword_search"]
        },
        {
            "name": "æ—¶é—´åºåˆ—å› æœåˆ†æ",
            "query": "æŸå…¬å¸è‚¡ä»·åœ¨å‘å¸ƒè´¢æŠ¥å‰ä¸€å‘¨å¼€å§‹ä¸Šæ¶¨ï¼Œè´¢æŠ¥å‘å¸ƒå½“å¤©ç»§ç»­ä¸Šæ¶¨ï¼Œä½†ç¬¬äºŒå¤©å¼€å§‹ä¸‹è·Œã€‚åŒæœŸï¼Œè¯¥å…¬å¸çš„ä¸»è¦ç«äº‰å¯¹æ‰‹å‘å¸ƒäº†æ–°äº§å“ã€‚è¯·åˆ†æè¿™ä¸€ç³»åˆ—äº‹ä»¶çš„å†…åœ¨é€»è¾‘å…³ç³»å’Œå¯èƒ½çš„å¸‚åœºé¢„æœŸå˜åŒ–ã€‚",
            "expected_tools": ["full_text_search", "vector_search"]
        },
        {
            "name": "æ¦‚å¿µè¾¹ç•Œæ¨¡ç³Šé—®é¢˜",
            "query": "åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œ'æœºå™¨å­¦ä¹ 'ã€'æ·±åº¦å­¦ä¹ 'å’Œ'ç¥ç»ç½‘ç»œ'è¿™ä¸‰ä¸ªæ¦‚å¿µç»å¸¸è¢«æ··ç”¨ã€‚è¯·åœ¨ç‰¹å®šè¯­å¢ƒä¸‹ï¼ˆæ¯”å¦‚æŸç¯‡è®ºæ–‡æˆ–æŸä¸ªäº§å“ä»‹ç»ä¸­ï¼‰å‡†ç¡®åŒºåˆ†å®ƒä»¬çš„å«ä¹‰ï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆåœ¨è¯¥è¯­å¢ƒä¸‹è¿™ç§åŒºåˆ†æ˜¯é‡è¦çš„ã€‚",
            "expected_tools": ["vector_search", "keyword_search"]
        },
        {
            "name": "å¤åˆè®¡ç®—éªŒè¯",
            "query": "æŸå…¬å¸å£°ç§°å…¶æ–°ç®—æ³•æ¯”ç°æœ‰æ–¹æ¡ˆæ•ˆç‡æå‡äº†300%ï¼ŒåŒæ—¶èƒ½è€—é™ä½äº†40%ï¼Œæˆæœ¬å‡å°‘äº†60%ã€‚è¯·æ ¹æ®ç›¸å…³æŠ€æœ¯æ–‡æ¡£éªŒè¯è¿™äº›æ•°æ®çš„ä¸€è‡´æ€§å’Œå¯ä¿¡åº¦ï¼Œå¹¶åˆ†ææ˜¯å¦å­˜åœ¨é€»è¾‘çŸ›ç›¾ã€‚",
            "expected_tools": ["full_text_search", "keyword_search"]
        },
        {
            "name": "è·¨å­¦ç§‘ç»¼åˆåˆ†æ",
            "query": "é‡å­è®¡ç®—çš„å‘å±•å¯¹ç°æœ‰çš„RSAåŠ å¯†ç®—æ³•æ„æˆå¨èƒï¼Œè¿™ç§å¨èƒå¯¹é‡‘èè¡Œä¸šçš„åŒºå—é“¾åº”ç”¨ä¼šäº§ç”Ÿä»€ä¹ˆå½±å“ï¼Ÿè¯·ä»æŠ€æœ¯ã€æ³•å¾‹ã€ç»æµä¸‰ä¸ªè§’åº¦è¿›è¡Œç»¼åˆåˆ†æã€‚",
            "expected_tools": ["full_text_search", "keyword_search", "vector_search"]
        },
        {
            "name": "éšå«å…³ç³»æ¨å¯¼",
            "query": "å¦‚æœå¼ ä¸‰æ˜¯ABCå…¬å¸çš„CTOï¼Œæå››æ˜¯XYZåŸºé‡‘çš„åˆä¼™äººï¼Œè€Œç‹äº”æ›¾åœ¨DEFå’¨è¯¢å…¬å¸å·¥ä½œè¿‡ï¼Œç°åœ¨ä»–ä»¬ä¸‰äººå…±åŒå‡ºç°åœ¨ä¸€ä»½å…³äºåŒºå—é“¾é¡¹ç›®çš„æŠ•èµ„åè®®ä¸­ã€‚è¯·æ¨æ–­è¿™ä¸ªé¡¹ç›®å¯èƒ½çš„æŠ€æœ¯ç‰¹ç‚¹ã€èµ„é‡‘è§„æ¨¡å’Œå¸‚åœºå®šä½ã€‚",
            "expected_tools": ["vector_search", "full_text_search"]
        },
        {
            "name": "å‡è®¾æƒ…æ™¯æ¨æ¼”",
            "query": "å‡è®¾æŸé¡¹æ–°æŠ€æœ¯ä½¿å¾—ç”µæ± èƒ½é‡å¯†åº¦æé«˜äº†10å€ï¼Œè¯·åˆ†æè¿™ä¸€çªç ´å¯¹ç”µåŠ¨æ±½è½¦äº§ä¸šé“¾ã€èƒ½æºç»“æ„ã€åŸå¸‚è§„åˆ’å¯èƒ½äº§ç”Ÿçš„è¿é”å½±å“ï¼Œå¹¶è¯„ä¼°å„ç§å½±å“å‘ç”Ÿçš„æ—¶é—´é¡ºåºå’Œæ¦‚ç‡ã€‚",
            "expected_tools": ["vector_search", "full_text_search", "keyword_search"]
        },
        {
            "name": "çº¯é€»è¾‘æ¨ç†æµ‹è¯•",
            "query": "å¦‚æœäº‹ä»¶Aå‘ç”Ÿçš„æ¦‚ç‡æ˜¯70%ï¼Œåœ¨Aå‘ç”Ÿçš„æ¡ä»¶ä¸‹Bå‘ç”Ÿçš„æ¦‚ç‡æ˜¯80%ï¼Œåœ¨Bå‘ç”Ÿçš„æ¡ä»¶ä¸‹Cå‘ç”Ÿçš„æ¦‚ç‡æ˜¯60%ã€‚ç°åœ¨å·²çŸ¥Cå‘ç”Ÿäº†ï¼Œè¯·è®¡ç®—Aå‘ç”Ÿçš„æ¦‚ç‡ï¼Œå¹¶è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹ã€‚",
            "expected_tools": []
        }
    ]
    
    # æ‰§è¡Œæµ‹è¯•
    total_tests = len(test_cases)
    passed_tests = 0
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'#'*100}")
        print(f"ğŸ§ª æµ‹è¯•ç”¨ä¾‹ {i}/{total_tests}: {test_case['name']}")
        print(f"{'#'*100}")
        
        try:
            start_time = time.time()
            response = simulate_conversation_with_tools(
                provider_name=provider_name,
                model=model,
                system_prompt=system_prompt,
                user_query=test_case['query'],
                tools=tools
            )
            end_time = time.time()
            
            # åˆ†æç»“æœ
            has_tool_calls = 'tool_calls' in response and response['tool_calls']
            used_tools = []
            if has_tool_calls:
                used_tools = [call.function.name for call in response['tool_calls']]
            
            print(f"\nğŸ“Š æµ‹è¯•ç»“æœåˆ†æ:")
            print(f"   â±ï¸  è€—æ—¶: {end_time - start_time:.2f} ç§’")
            print(f"   ğŸ”§ ä½¿ç”¨çš„å·¥å…·: {used_tools}")
            print(f"   ğŸ¯ æœŸæœ›çš„å·¥å…·: {test_case['expected_tools']}")
            print(f"   ğŸ“Š è¾“å…¥token: {response.get('input_tokens', 0)}")
            print(f"   ğŸ“¤ è¾“å‡ºtoken: {response.get('output_tokens', 0)}")
            
            # ç®€å•çš„æµ‹è¯•éªŒè¯
            if not test_case['expected_tools']:
                # æœŸæœ›æ— å·¥å…·è°ƒç”¨
                if not has_tool_calls:
                    print("   âœ… æµ‹è¯•é€šè¿‡: æ­£ç¡®åœ°æ²¡æœ‰ä½¿ç”¨å·¥å…·")
                    passed_tests += 1
                else:
                    print("   âŒ æµ‹è¯•å¤±è´¥: ä¸åº”è¯¥ä½¿ç”¨å·¥å…·ä½†å´ä½¿ç”¨äº†")
            else:
                # æœŸæœ›æœ‰å·¥å…·è°ƒç”¨
                if has_tool_calls:
                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æœŸæœ›çš„å·¥å…·
                    expected_set = set(test_case['expected_tools'])
                    used_set = set(used_tools)
                    if used_set.intersection(expected_set):
                        print("   âœ… æµ‹è¯•é€šè¿‡: ä½¿ç”¨äº†æœŸæœ›çš„å·¥å…·")
                        passed_tests += 1
                    else:
                        print("   âš ï¸  æµ‹è¯•éƒ¨åˆ†é€šè¿‡: ä½¿ç”¨äº†å·¥å…·ä½†ä¸æ˜¯æœŸæœ›çš„å·¥å…·")
                        passed_tests += 0.5
                else:
                    print("   âŒ æµ‹è¯•å¤±è´¥: åº”è¯¥ä½¿ç”¨å·¥å…·ä½†æ²¡æœ‰ä½¿ç”¨")
                    
        except Exception as e:
            print(f"   âŒ æµ‹è¯•å¼‚å¸¸: {e}")
    
    # æ€»ç»“
    print(f"\n{'='*100}")
    print(f"ğŸ† æµ‹è¯•æ€»ç»“")
    print(f"{'='*100}")
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
    elif passed_tests >= total_tests * 0.8:
        print("ğŸ‘ å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼ŒAgentè¡¨ç°è‰¯å¥½")
    else:
        print("âš ï¸  éœ€è¦æ”¹è¿›Agentçš„å·¥å…·è°ƒç”¨é€»è¾‘")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Agent Function Call æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    try:
        run_agent_tests()
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ Agentæµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()
